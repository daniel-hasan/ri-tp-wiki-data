<html><title>Teoria da informa&ccedil;&atilde;o</title><body>&#10;<p> &#10;A <b>Teoria da informa&ccedil;&atilde;o</b> ou <b><a href="http://en.wikipedia.org/wiki/Teoria_matem%C3%A1tica_da_comunica%C3%A7%C3%A3o" id="w">Teoria matem&aacute;tica da comunica&ccedil;&atilde;o</a></b> &eacute; um  ramo da teoria da <a href="http://en.wikipedia.org/wiki/Probabilidade" id="w">probabilidade</a> e da matem&aacute;tica <a href="http://en.wikipedia.org/wiki/Estat%C3%ADstica" id="w">estat&iacute;stica</a> que lida com sistemas de <a href="http://en.wikipedia.org/wiki/Comunica%C3%A7%C3%A3o" id="w">comunica&ccedil;&atilde;o</a>, <a href="http://en.wikipedia.org/wiki/Transmiss%C3%A3o_de_dados" id="w">transmiss&atilde;o de dados</a>, <a href="http://en.wikipedia.org/wiki/Criptografia" id="w">criptografia</a>, <a href="http://en.wikipedia.org/wiki/Codifica%C3%A7%C3%A3o" id="w">codifica&ccedil;&atilde;o</a>, teoria do ru&iacute;do, corre&ccedil;&atilde;o de erros, <a href="http://en.wikipedia.org/wiki/Compress%C3%A3o_de_dados" id="w">compress&atilde;o de dados</a>, etc. Ela n&atilde;o deve ser confundida com <a href="http://en.wikipedia.org/wiki/Tecnologia_da_informa%C3%A7%C3%A3o" id="w">tecnologia da informa&ccedil;&atilde;o</a> e <a href="http://en.wikipedia.org/wiki/Biblioteconomia" id="w">biblioteconomia</a>.</p>&#10;<p><a href="http://en.wikipedia.org/wiki/Claude_Shannon" id="w">Claude Shannon</a> (1916&shy;2001) &eacute; conhecido como &#34&semi;o pai da teoria  da informa&ccedil;&atilde;o&#34&semi;. Sua teoria foi a primeira a considerar comunica&ccedil;&atilde;o como um problema matem&aacute;tico rigorosamente embasado na estat&iacute;stica e deu aos engenheiros da comunica&ccedil;&atilde;o um modo de determinar a capacidade de um canal de comunica&ccedil;&atilde;o em termos de ocorr&ecirc;ncia de <a href="http://en.wikipedia.org/wiki/Bit" id="w">bits</a>. A teoria n&atilde;o se preocupa com a sem&acirc;ntica dos dados, mas pode envolver aspectos relacionados com a perda de informa&ccedil;&atilde;o na compress&atilde;o e na transmiss&atilde;o de mensagens com ru&iacute;do no canal.</p>&#10;<p>&Eacute; geralmente aceito que a moderna disciplina da <b>teoria da informa&ccedil;&atilde;o</b>&#10;come&ccedil;ou com duas publica&ccedil;&otilde;es: a do artigo cient&iacute;fico de <a href="http://en.wikipedia.org/wiki/Claude_Shannon" id="w">Shannon</a> intitulado <i><a href="http://en.wikipedia.org/wiki/Teoria_matem%C3%A1tica_da_comunica%C3%A7%C3%A3o" id="w">Teoria Matem&aacute;tica da Comunica&ccedil;&atilde;o</a></i> (<a class="externallink" href="http://cm.bell&shy;labs.com/cm/ms/what/shannonday/shannon1948.pdf" rel="nofollow" title="http://cm.bell&shy;labs.com/cm/ms/what/shannonday/shannon1948.pdf">&#39&semi;&#39&semi;&#34&semi;A Mathematical Theory of Communication&#34&semi;&#39&semi;&#39&semi;</a>), no <a href="http://en.wikipedia.org/wiki/Bell_System_Technical_Journal" id="w">Bell System Technical Journal</a>, em julho e outubro de <a href="http://en.wikipedia.org/wiki/1948" id="w">1948</a>&semi; e do livro de <a href="http://en.wikipedia.org/wiki/Claude_Shannon" id="w">Shannon</a> em co&shy;autoria com o tamb&eacute;m <a href="http://en.wikipedia.org/wiki/Engenharia" id="w">engenheiro</a> <a href="http://en.wikipedia.org/wiki/Estados_Unidos" id="w">estadunidense</a> <a href="http://en.wikipedia.org/wiki/Warren_Weaver" id="w">Warren Weaver</a> (<a href="http://en.wikipedia.org/wiki/1894" id="w">1894</a>&shy;<a href="http://en.wikipedia.org/wiki/1978" id="w">1978</a>), intitulado <i><a href="http://en.wikipedia.org/wiki/Teoria_matem%C3%A1tica_da_comunica%C3%A7%C3%A3o" id="w">Teoria Matem&aacute;tica da Comunica&ccedil;&atilde;o</a></i> (<i>The Mathematical Theory of Communication</i>), e contendo reimpress&otilde;es do artigo cient&iacute;fico anterior de forma acess&iacute;vel tamb&eacute;m a n&atilde;o&shy;especialistas &shy; isto popularizou os conceitos.</p>&#10;<a id="Contexto_hist%C3%B3rico" name="Contexto_hist%C3%B3rico"></a><h2> Contexto hist&oacute;rico </h2>&#10;<p>O marco que estabeleceu a teoria da informa&ccedil;&atilde;o e chamou imediatamente a aten&ccedil;&atilde;o mundial foi o artigo <i>A Mathematical Theory of Communication</i> escrito por Claude Shannon de julho a outubro de 1948.</p>&#10;<p>Antes deste artigo, algumas abordagens te&oacute;ricas ainda que limitadas vinham sendo desenvolvidas nos laborat&oacute;rios da Bell, todas implicitamente assumindo eventos de igual probabilidade. O artigo <i>Certain Factors Affecting Telegraph Speed</i> de <a href="http://en.wikipedia.org/wiki/Harry_Nyquist" id="w">Harry Nyquist</a> escrito em 1924 cont&eacute;m uma se&ccedil;&atilde;o te&oacute;rica que quantifica <i>intelig&ecirc;ncia</i> e a velocidade de transmiss&atilde;o pela qual ela pode ser transmitida por um sistema de comunica&ccedil;&atilde;o,  estabelecendo a rela&ccedil;&atilde;o <span class="math"> W = K \log{m}</span>, onde <span class="math">W</span> &eacute; a velocidade de transmiss&atilde;o da intelig&ecirc;ncia, <span class="math">m</span> &eacute; o n&uacute;mero de n&iacute;veis de tens&atilde;o para cada intervalo de tempo, e <span class="math">K</span> &eacute; uma constante. Em 1928, <a href="http://en.wikipedia.org/wiki/Ralph_Hartley" id="w">Ralph Hartley</a> publicou o artigo <i>Transmission of Information</i>, onde aparece a palavra <i>informa&ccedil;&atilde;o</i> como uma quantidade mensur&aacute;vel que a capacidade do destinat&aacute;rio distinguir diferentes sequ&ecirc;ncias de s&iacute;mbolos, levando &agrave; express&atilde;o <span class="math">H = \log{S^n} = n \log{S}</span>, onde <span class="math">S</span> e <span class="math">n</span> representam, respectivamente, o n&uacute;mero de s&iacute;mbolos poss&iacute;veis e o n&uacute;mero de s&iacute;mbolos na transmiss&atilde;o. Inicialmente, a unidade natural da transmiss&atilde;o foi definida como sendo o d&iacute;gito decimal, sendo, posteriormente, renomeada para hartley em uma clara homenagem. <a href="http://en.wikipedia.org/wiki/Alan_Turing" id="w">Alan Turing</a> em 1940, durante a 2&ordf; Guerra Mundial, aplicou ideias similares como parte da an&aacute;lise estat&iacute;stica para decifrar a criptografia da m&aacute;quina alem&atilde; Enigma.</p>&#10;<p>Boa parte da matem&aacute;tica por tr&aacute;s da teoria da informa&ccedil;&atilde;o com eventos de diferentes probabilidades foi desenvolvida para os campos da <a href="http://en.wikipedia.org/wiki/Termodin%C3%A2mica" id="w">termodin&acirc;mica</a> por <a href="http://en.wikipedia.org/wiki/Ludwig_Boltzmann" id="w">Ludwig Boltzmann</a> e <a href="http://en.wikipedia.org/wiki/J._Willard_Gibbs" id="w">J. Willard Gibbs</a>. As conex&otilde;es entre as entropias da informa&ccedil;&atilde;o e termodin&acirc;mica, incluindo as importantes contribui&ccedil;&otilde;es de <a href="http://en.wikipedia.org/wiki/Rolf_Landauer" id="w">Rolf Landauer</a> na d&eacute;cada de 1960, s&atilde;o exploradas na <a href="http://en.wikipedia.org/wiki/Entropia_termodin%C3%A2mica_e_teoria_da_informa%C3%A7%C3%A3o" id="w">Entropia termodin&acirc;mica e teoria da informa&ccedil;&atilde;o</a>.</p>&#10;<p>No artigo seminal de Shannon, introduz&shy;se pela primeira vez um modelo quantitativo e qualitativo da comunica&ccedil;&atilde;o, apresentando&shy;a como um processo estat&iacute;stico subjacente &agrave; teoria da informa&ccedil;&atilde;o. Shannon inicia seu artigo dizendo</p>&#10;<p>&#34&semi;O problema fundamental da comunica&ccedil;&atilde;o &eacute; reproduzir em um dado ponto, exata ou aproximadamente, uma mensagem produzida em outro ponto.&#34&semi;</p>&#10;<p>Com este artigo vieram &agrave; tona os conceitos&#10;</p>&#10;<ul>&#10;<li>de <a href="http://en.wikipedia.org/wiki/Entropia_da_informa%C3%A7%C3%A3o" id="w">entropia da informa&ccedil;&atilde;o</a> e <a href="http://en.wikipedia.org/wiki/Redund%C3%A2ncia" id="w">redund&acirc;ncia</a> de uma fonte, e sua aplica&ccedil;&atilde;o no <a href="http://en.wikipedia.org/wiki/Teorema_de_codifica%C3%A7%C3%A3o_da_fonte" id="w">teorema de codifica&ccedil;&atilde;o da fonte</a>&semi;</li>&#10;<li>de <a href="http://en.wikipedia.org/wiki/Informa%C3%A7%C3%A3o_m%C3%BAtua" id="w">informa&ccedil;&atilde;o m&uacute;tua</a> e <a href="http://en.wikipedia.org/wiki/Capacidade_de_um_canal_com_ru%C3%ADdo" id="w">capacidade de um canal com ru&iacute;do</a>, incluindo a promessa de comunica&ccedil;&atilde;o sem perdas estabelecida no teorema de <a href="http://en.wikipedia.org/wiki/Codifica%C3%A7%C3%A3o_de_canais&shy;ruidosos" id="w">codifica&ccedil;&atilde;o de canais&shy;ruidosos</a>&semi;</li>&#10;<li>da <a href="http://en.wikipedia.org/wiki/Lei_de_Shannon&shy;Hartley" id="w">lei de Shannon&shy;Hartley</a> para a capacidade de um <a href="http://en.wikipedia.org/wiki/Canal_Gaussiano" id="w">canal Gaussiano</a>&semi;</li>&#10;<li>do <a href="http://en.wikipedia.org/wiki/Bit" id="w">bit</a> &shy; uma nova forma de enxergar a unidade fundamental da informa&ccedil;&atilde;o.</li></ul>&#10;<a id="Entropia_da_informa%C3%A7%C3%A3o" name="Entropia_da_informa%C3%A7%C3%A3o"></a><h2> Entropia da informa&ccedil;&atilde;o </h2>&#10;<p>No processo de desenvolvimento de uma teoria da comunica&ccedil;&atilde;o que pudesse ser aplicada por engenheiros eletricistas para projetar sistemas de telecomunica&ccedil;&atilde;o melhores, <a href="http://en.wikipedia.org/wiki/Claude_Shannon" id="w">Shannon</a> definiu uma medida chamada de <a href="http://en.wikipedia.org/wiki/Entropia_(teoria_da_informa%C3%A7%C3%A3o)" id="w">entropia</a>, definida como:</p>&#10;<dl><dd><span class="math"> H(X) = &shy;\sum_{x \in \mathbb{X}} p(x) \log p(x)</span></dd></dl>&#10;<p>onde <big><span class="math">log</span></big>  &eacute; o logaritmo na base 2, que determina o grau de caoticidade da distribui&ccedil;&atilde;o de probabilidade <span class="math">p_i</span> e pode ser usada para determinar a capacidade do canal necess&aacute;ria para transmitir a informa&ccedil;&atilde;o.</p>&#10;<p>A medida de entropia de Shannon passou a ser considerada como uma medida da <i>informa&ccedil;&atilde;o</i> contida numa mensagem, em oposi&ccedil;&atilde;o &agrave; parte da mensagem que &eacute; estritamente determinada (portanto prev&iacute;sivel) por estruturas inerentes, como por exemplo a redund&acirc;ncia da estrutura das linguagens ou das propriedades estat&iacute;sticas de uma linguagem, relacionadas &agrave;s frequ&ecirc;ncias de ocorr&ecirc;ncia de diferentes letras (<a href="http://en.wikipedia.org/wiki/Monema" id="w">monemas</a>) ou de pares, trios, (<a href="http://en.wikipedia.org/wiki/Fonema" id="w">fonemas</a>) etc., de palavras. Veja <a href="http://en.wikipedia.org/wiki/Cadeia_de_Markov" id="w">cadeia de Markov</a>.</p>&#10;<p>A entropia como definida por Shannon est&aacute; intimamente relacionada &agrave; entropia definida por f&iacute;sicos. <a href="http://en.wikipedia.org/wiki/Boltzmann" id="w">Boltzmann</a> e <a href="http://en.wikipedia.org/wiki/Gibbs" id="w">Gibbs</a> fizeram um trabalho consider&aacute;vel sobre termodin&acirc;mica estat&iacute;stica. Este trabalho foi a inspira&ccedil;&atilde;o para se adotar o termo entropia em teoria da informa&ccedil;&atilde;o. H&aacute; uma profunda rela&ccedil;&atilde;o entre entropia nos sentidos termodin&acirc;mico e informacional. Por exemplo, o <a href="http://en.wikipedia.org/wiki/Dem%C3%B4nio_de_Maxwell" id="w">dem&ocirc;nio de Maxwell</a> necessita de informa&ccedil;&otilde;es para reverter a entropia termodin&acirc;mica e a obten&ccedil;&atilde;o dessas informa&ccedil;&otilde;es equilibra exatamente o ganho termodin&acirc;mico que o dem&ocirc;nio alcan&ccedil;aria de outro modo.</p>&#10;<p>Outras medidas de informa&ccedil;&atilde;o &uacute;teis incluem informa&ccedil;&atilde;o m&uacute;tua, que &eacute; uma medida da correla&ccedil;&atilde;o entre dois conjuntos de eventos. Informa&ccedil;&atilde;o m&uacute;tua &eacute; definida para duas vari&aacute;veis aleat&oacute;rias <i>X</i> e <i>Y</i> como:</p>&#10;<dl><dd><span class="math">\ M(X,Y)=H(X,Y)&shy;H(X)&shy;H(Y)</span></dd></dl>&#10;<p>onde <span class="math">H(X,Y)</span> &eacute; a entropia conjunta (<i>joint entropy</i>) ou</p>&#10;<dl><dd><span class="math">\ H(X,Y)=&shy;\sum_{x,y} p(x,y)\log p(x,y)</span></dd></dl>&#10;<p>Informa&ccedil;&atilde;o m&uacute;tua est&aacute; relacionada de forma muito pr&oacute;xima com testes estat&iacute;sticos como o teste de raz&atilde;o logar&iacute;tmica e o teste Chi&shy;square.</p>&#10;<p>A teoria da informa&ccedil;&atilde;o de Shannon &eacute; apropriada para medir incerteza sobre um espa&ccedil;o desordenado. Uma medida alternativa de informa&ccedil;&atilde;o foi criada por Fisher para medir incerteza sobre um espa&ccedil;o ordenado. Por exemplo, a informa&ccedil;&atilde;o de Shannon &eacute; usada sobre um espa&ccedil;o de letras do alfabeto, j&aacute; que letras n&atilde;o tem &#39&semi;dist&acirc;ncias&#39&semi; entre elas. Para informa&ccedil;&atilde;o sobre valores de par&acirc;metros cont&iacute;nuos, como as alturas de pessoas, a informa&ccedil;&atilde;o de Fisher &eacute; usada, j&aacute; que tamanhos estimados tem uma dist&acirc;ncia bem definida.</p>&#10;<p>Diferen&ccedil;as na informa&ccedil;&atilde;o de Shannon correspondem a um caso especial da dist&acirc;ncia de Kullback&shy;Leibler da estat&iacute;stica Bayesiana, uma medida de dist&acirc;ncia entre distribui&ccedil;&otilde;es de probabilidade a priori e a posteriori.</p>&#10;<p><a href="http://en.wikipedia.org/wiki/Andrei_Nikolaevich_Kolmogorov" id="w">Andrei Nikolaevich Kolmogorov</a> introduziu uma medida de informa&ccedil;&atilde;o que &eacute; baseada no menor algoritmo que pode comput&aacute;&shy;la (veja <a href="http://en.wikipedia.org/wiki/Complexidade_de_Kolmogorov" id="w">complexidade de Kolmogorov</a>).</p>&#10;<a id="Ver_tamb%C3%A9m" name="Ver_tamb%C3%A9m"></a><h2>Ver tamb&eacute;m</h2>&#10;&#10;<ul>&#10;<li><a href="http://en.wikipedia.org/wiki/Abraham_Moles" id="w">Abraham Moles</a></li>&#10;<li><a href="http://en.wikipedia.org/wiki/Dist%C3%A2ncia_Levenshtein" id="w">Dist&acirc;ncia Levenshtein</a></li>&#10;<li><a href="http://en.wikipedia.org/wiki/Informa%C3%A7%C3%A3o" id="w">Informa&ccedil;&atilde;o</a></li>&#10;<li><a href="http://en.wikipedia.org/wiki/Norbert_Wiener" id="w">Norbert Wiener</a></li></ul>&#10;<a id="Liga%C3%A7%C3%B5es_externas" name="Liga%C3%A7%C3%B5es_externas"></a><h2>Liga&ccedil;&otilde;es externas</h2>&#10;&#10;<ul>&#10;<li> </li>&#10;<li> </li>&#10;<li> </li></ul>&#10;&#10;<p> &#10; </p>&#10;<p> &#10;<a href="http://en.wikipedia.org/wiki/Categoria:Teoria_da_informa%C3%A7%C3%A3o" id="w">*</a>&#10;<a href="http://en.wikipedia.org/wiki/Categoria:Teoria_das_probabilidades" id="w">Categoria:Teoria das probabilidades</a>&#10;<a href="http://en.wikipedia.org/wiki/Categoria:Comunica%C3%A7%C3%A3o" id="w">Categoria:Comunica&ccedil;&atilde;o</a>&#10;<a href="http://en.wikipedia.org/wiki/Categoria:Cibern%C3%A9tica" id="w">Categoria:Cibern&eacute;tica</a>&#10;<a href="http://en.wikipedia.org/wiki/Categoria:Ci%C3%AAncias_formais" id="w">Categoria:Ci&ecirc;ncias formais</a></p></body></html>