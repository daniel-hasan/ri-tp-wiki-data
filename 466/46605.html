<html><title>Complexidade de Kolmogorov</title><body>&#10;<p>A <b>complexidade de Kolmogorov</b> &eacute; uma <a href="http://en.wikipedia.org/wiki/Teoria_da_informa%C3%A7%C3%A3o" id="w">teoria da informa&ccedil;&atilde;o</a> e da <a href="http://en.wikipedia.org/wiki/Aleatoriedade" id="w">aleatoriedade</a>, profunda e sofisticada, que trata da quantidade de <a href="http://en.wikipedia.org/wiki/Informa%C3%A7%C3%A3o" id="w">informa&ccedil;&atilde;o</a> de objetos individuais, medida atrav&eacute;s do tamanho de sua menor descri&ccedil;&atilde;o <a href="http://en.wikipedia.org/wiki/Algoritmo" id="w">algor&iacute;tmica</a>. Ela &eacute; uma no&ccedil;&atilde;o moderna de aleatoriedade, e refere&shy;se a um conceito pontual de aleatoriedade, ao inv&eacute;s de uma aleatoriedade m&eacute;dia como o faz a <a href="http://en.wikipedia.org/wiki/Teoria_das_probabilidades" id="w">teoria das probabilidades</a>. Ela &eacute; um ramo derivado da <a href="http://en.wikipedia.org/wiki/Teoria_da_informa%C3%A7%C3%A3o" id="w">teoria da informa&ccedil;&atilde;o</a> de <a href="http://en.wikipedia.org/wiki/Claude_Shannon" id="w">Claude Shannon</a>, embora hoje possa ser considerada uma &aacute;rea de pesquisa madura e aut&ocirc;noma.</p>&#10;<p>Complexidade de Kolmogorov define uma nova <a href="http://en.wikipedia.org/wiki/Teoria" id="w">teoria</a> da informa&ccedil;&atilde;o, chamada <b>teoria algor&iacute;tmica da informa&ccedil;&atilde;o</b> (por tratar com <a href="http://en.wikipedia.org/wiki/Algoritmo" id="w">algoritmos</a>). A <a href="http://en.wikipedia.org/wiki/M%C3%A1quina_de_Turing" id="w">M&aacute;quina de Turing</a> &eacute; usada como mecanismo para descrever os objetos (para definir os algoritmos).</p>&#10;<a id="Origem" name="Origem"></a><h2> Origem </h2>&#10;<p>A complexidade de Kolmogorov tem sua origem nos anos <a href="http://en.wikipedia.org/wiki/1960" id="w">1960</a>, quando <a href="http://en.wikipedia.org/wiki/Andrei_Nikolaevich_Kolmogorov" id="w">Andrei Kolmogorov</a>, <a href="http://en.wikipedia.org/wiki/Ray_Solomonoff" id="w">Ray Solomonoff</a> e <a href="http://en.wikipedia.org/wiki/Gregory_Chaitin" id="w">Gregory Chaitin</a> desenvolveram, de forma independente, uma teoria da informa&ccedil;&atilde;o baseada no tamanho dos programas para a <a href="http://en.wikipedia.org/wiki/M%C3%A1quina_de_Turing" id="w">M&aacute;quina de Turing</a>. Convencionou&shy;se chamar a &aacute;rea genericamente de &#34&semi;complexidade de Kolmogorov&#34&semi; em homenagem ao famoso matem&aacute;tico <a href="http://en.wikipedia.org/wiki/R%C3%BAssia" id="w">russo</a>, e fundador da &aacute;rea, <a href="http://en.wikipedia.org/wiki/Andrei_Nikolaevich_Kolmogorov" id="w">Andrei Nikolaevich Kolmogorov</a> (<a href="http://en.wikipedia.org/wiki/1903" id="w">1903</a>&shy;<a href="http://en.wikipedia.org/wiki/1987" id="w">1987</a>).</p>&#10;<a id="Defini%C3%A7%C3%A3o" name="Defini%C3%A7%C3%A3o"></a><h2> Defini&ccedil;&atilde;o </h2>&#10;<p>A complexidade de Kolmogorov est&aacute; definida sobre o conjunto de strings bin&aacute;rias (seq&uuml;&ecirc;ncias de zeros e uns). Ela associa a cada string bin&aacute;ria um valor num&eacute;rico que &eacute; sua &#34&semi;complexidade&#34&semi;.</p>&#10;<p>A complexidade de Kolmogorov pode ser definida simplificadamente como o tamanho do menor programa (ou descri&ccedil;&atilde;o algoritmica) que computa na M&aacute;quina de Turing uma determinada string bin&aacute;ria.</p>&#10;<p>Exige&shy;se que o conjunto de descri&ccedil;&otilde;es (programas) forme um conjunto livre de prefixo, e se chama esta complexidade de &#34&semi;complexidade de prefixo&#34&semi;, representada por <span class="math">K(x)</span>. Para enfatizar a M&aacute;quina que est&aacute; sendo usada para definir a complexidade podemos escrever <span class="math">K_M(x)</span> ao inv&eacute;s de <span class="math">K(x)</span>, onde <span class="math">M</span> &eacute; uma M&aacute;quina de Turing em particular.</p>&#10;<p>Tamb&eacute;m podemos definir uma complexidade condicional, <span class="math">K(x|y)</span>, definida como o tamanho do programa que computa <span class="math">x</span> a partir de <span class="math">y</span>, que &eacute; dado como entrada do programa. Intuitivamente, <span class="math">K(x|y)</span> representa a quantidade de informa&ccedil;&atilde;o que devemos adicionar &agrave; informa&ccedil;&atilde;o em <span class="math">y</span> para computar <span class="math">x</span>.</p>&#10;<p>Importante que o conceito de descri&ccedil;&atilde;o seja bem fundamentado (ou seja, formalmente definido como um programa para a m&aacute;quina de Turing) para evitar paradoxos. Paradoxos (ou antin&ocirc;mias) s&atilde;o bastante freq&uuml;entes na hist&oacute;ria da matem&aacute;tica. Podemos citar o famoso &#34&semi;paradoxo do barbeiro&#34&semi;, de autoria de <a href="http://en.wikipedia.org/wiki/Bertrand_Russell" id="w">Bertrand Russel</a>, que diz que &#34&semi;o barbeiro &eacute; aquele que barbeia os homens que n&atilde;o barbeiam a si pr&oacute;prios&#34&semi;. O paradoxo &eacute; obtido ao perguntar se o barbeiro barbeia a si pr&oacute;prio.</p>&#10;<a id="M%C3%A1quina_de_Turing" name="M%C3%A1quina_de_Turing"></a><h2> M&aacute;quina de Turing </h2>&#10;<p> &#10;A M&aacute;quina de Turing &eacute; um dispositivo que foi proposto em <a href="http://en.wikipedia.org/wiki/1936" id="w">1936</a> por <a href="http://en.wikipedia.org/wiki/Alan_Turing" id="w">Alan Turing</a> com o objetivo de formalizar matematicamente o que entendemos por &#34&semi;computador&#34&semi;. Ela &eacute; basicamente formada por uma fita na qual um cabe&ccedil;ote pode escrever s&iacute;mbolos de um conjunto pr&eacute;&shy;definido, e assim c&aacute;lculos de diversos tipos podem ser feitos apenas com os movimentos deste cabe&ccedil;ote sobre a fita.</p>&#10;<p>Uma propriedade importante da M&aacute;quina de Turing &eacute; a <i>universalidade</i>, ou seja, a capacidade da M&aacute;quina de Turing de <i>simular</i> qualquer outra m&aacute;quina. Por isto, podemos chama&shy;la de <i>m&aacute;quina universal</i>.</p>&#10;<a id="Teorema_da_Invari%C3%A2ncia" name="Teorema_da_Invari%C3%A2ncia"></a><h2> Teorema da Invari&acirc;ncia </h2>&#10;<p>Da forma como foi definido, pode parecer que a complexidade de Kolmogorov permanece dependente da m&aacute;quina escolhida como m&aacute;quina de refer&ecirc;ncia. No entanto, existe um teorema (chamado Teorema da Invari&acirc;ncia) que prova que todas as m&aacute;quinas universais (ou seja, que tem a capacidade de simular todas as outras m&aacute;quinas de Turing conceb&iacute;veis) resultam no mesmo valor de complexidade, a n&atilde;o ser por uma constante que &eacute; assintoticamente desprez&aacute;vel:&#10;<span class="math">K_{U1}(x)\leq K_{U2}(x)+c</span>, para alguma constante <span class="math">c</span>, e&#10;<span class="math">U1</span> e <span class="math">U2</span> s&atilde;o duas m&aacute;quinas universais quaisquer.</p>&#10;<p>Isto significa que a complexidade passa a ser um atributo intr&iacute;nseco do objeto, independente da m&aacute;quina universal escolhida como m&aacute;quina de refer&ecirc;ncia.</p>&#10;<a id="Princ%C3%ADpio_de_Occam" name="Princ%C3%ADpio_de_Occam"></a><h2> Princ&iacute;pio de Occam </h2>&#10;<p> &#10;Ao medir a complexidade de uma string pelo tamanho do menor programa, a complexidade de Kolmogorov reedita um antigo princ&iacute;pio, de autoria de <a href="http://en.wikipedia.org/wiki/William_de_Ockham" id="w">William de Ockham</a> (<a href="http://en.wikipedia.org/wiki/1285" id="w">1285</a>?&shy;<a href="http://en.wikipedia.org/wiki/1349" id="w">1349</a>?), que ficou conhecido como <a href="http://en.wikipedia.org/wiki/Navalha_de_Occam" id="w">Navalha de Occam</a>, que diz que &#34&semi;entidades n&atilde;o devem ser multiplicadas desnecessariamente&#34&semi;.</p>&#10;<p>O Princ&iacute;pio de Occam significa que entre todas as teorias que representam um fen&ocirc;meno, a mais simples (ou menor) &eacute; a melhor para este fim. Isto tem evidentes conota&ccedil;&otilde;es filos&oacute;ficas relacionadas com a expressividade dos m&eacute;todos de descri&ccedil;&atilde;o e com os sistemas formais.</p>&#10;<a id="Algumas_propriedades" name="Algumas_propriedades"></a><h2> Algumas propriedades </h2>&#10;<p>Trivialmente, <span class="math">K(x)\leq |x|+c</span>, para algum <span class="math">c</span>, onde <span class="math">|x|</span> denota o tamanho da string <span class="math">x</span>, j&aacute; que qualquer string pode ser computada por um programa que a tem armazenada literalmente no programa.</p>&#10;<p>Tamb&eacute;m sabemos que <span class="math">K(x|y)\leq K(x)+c</span>, j&aacute; que, na complexidade condicional, a presen&ccedil;a de <span class="math">y</span> &eacute; uma informa&ccedil;&atilde;o a mais que &eacute; fornecida ao programa que computar&aacute; <span class="math">x</span>.</p>&#10;<p>Se <span class="math">x</span> denota uma string bin&aacute;ria que &eacute; resultado de um <span class="math">p</span>&shy;processo de <a href="http://en.wikipedia.org/wiki/Bernoulli" id="w">Bernoulli</a>, ent&atilde;o <span class="math">\frac{1}{n}K(x)\rightarrow H(p,1&shy;p)</span> em probabilidade, onde <span class="math">H(p,1&shy;p)</span> &eacute; a <a href="http://en.wikipedia.org/wiki/Entropia" id="w">entropia</a> do processo. Isto significa que <span class="math">K(x)</span> &eacute; assintoticamente convergente &agrave; entropia da teoria da informa&ccedil;&atilde;o cl&aacute;ssica.</p>&#10;<a id="Incompressividade_de_strings_bin%C3%A1rias" name="Incompressividade_de_strings_bin%C3%A1rias"></a><h2> Incompressividade de strings bin&aacute;rias </h2>&#10;<p>Se para uma string bin&aacute;ria <span class="math">x</span>, <span class="math">K(x)\geq |x|&shy;c</span>, para algum <span class="math">c</span>, ent&atilde;o dizemos que <span class="math">x</span> &eacute; incompress&iacute;vel.</p>&#10;<p>Kolmogorov associou o conceito de incompressividade com o conceito de aleatoriedade,  com o objetivo de definir <i>seq&uuml;&ecirc;ncias aleat&oacute;rias</i>, e assim fornecer uma base te&oacute;rica para <a href="http://en.wikipedia.org/wiki/Probabilidade" id="w">probabilidades</a> e <a href="http://en.wikipedia.org/wiki/Estat%C3%ADstica" id="w">estat&iacute;stica</a>.</p>&#10;<a id="Objetivo_da_complexidade_de_Kolmogorov" name="Objetivo_da_complexidade_de_Kolmogorov"></a><h2> Objetivo da complexidade de Kolmogorov </h2>&#10;<p>O objetivo original do trabalho de Kolmogorov era obter uma defini&ccedil;&atilde;o formal de seq&uuml;&ecirc;ncia aleat&oacute;ria. Kolmogorov observou que algumas seq&uuml;&ecirc;ncias bin&aacute;rias podiam ser comprimidas algoritmicamente. Nos dias de hoje, a maioria das pessoas est&aacute; acostumada a usar programas de compress&atilde;o de dados, tais como <a href="http://en.wikipedia.org/wiki/Winzip" id="w">winzip</a>, <a href="http://en.wikipedia.org/wiki/Gzip" id="w">gzip</a>, <a href="http://en.wikipedia.org/wiki/Arj" id="w">arj</a>, etc. Assim, a id&eacute;ia que dados possam ser comprimidos n&atilde;o &eacute; de todo estranha.</p>&#10;<p>Fica f&aacute;cil perceber que um n&uacute;mero como 1.000.000.000 na base 10 pode ser facilmente expresso como <span class="math">10^9</span>, enquanto que o n&uacute;mero 5.321.478.927, escolhido arbitrariamente, n&atilde;o poderia ser expresso desta forma compacta.</p>&#10;<p>Se tomarmos as strings bin&aacute;rias 11111111111111111111 e 10110011010111011010, aceitar&iacute;amos facilmente a segunda como sendo resultante do lan&ccedil;amento sucessivo de uma moeda honesta (1=cara e 0=coroa). No entanto, dificilmente a primeira seq&uuml;&ecirc;ncia poderia ser resultado de um experimento realmente aleat&oacute;rio. Notamos que a primeira string poderia ser computada pelo programa:</p>&#10;<p>FOR I=1 TO 20 PRINT 1</p>&#10;<p>Que &eacute; um programa pequeno em rela&ccedil;&atilde;o ao tamanho da string. J&aacute; a segunda string n&atilde;o pode ser computada por um programa t&atilde;o curto, pois ele dever&aacute; conter a pr&oacute;pria string literalmente armazenada dentro do programa:</p>&#10;<p>PRINT 10110011010111011010</p>&#10;<p>Kolmogorov postulou que seq&uuml;&ecirc;ncias que n&atilde;o podem ser comprimidas algoritmicamente em uma descri&ccedil;&atilde;o de tamanho muito menor que a seq&uuml;&ecirc;ncia original s&atilde;o seq&uuml;&ecirc;ncias aleat&oacute;rias (no sentido estoc&aacute;stico do termo). Esta defini&ccedil;&atilde;o s&oacute; funciona se forem admitidas apenas descri&ccedil;&otilde;es dentro de uma classe de descri&ccedil;&otilde;es livres de prefixo (ou seja que nenhuma descri&ccedil;&atilde;o seja prefixo de outra da classe).</p>&#10;<a id="Desigualdade_de_Kraft" name="Desigualdade_de_Kraft"></a><h2> Desigualdade de Kraft </h2>&#10;<p>A raz&atilde;o para exigir que o conjunto de descri&ccedil;&otilde;es (programas) forme um conjunto livre de prefixo deve&shy;se a desigualdade de Kraft, express&atilde;o bem conhecida na teoria da informa&ccedil;&atilde;o cl&aacute;ssica. Ela significa que a s&eacute;rie <span class="math">\sum 2^{&shy;K(x)}</span> converge. Esta &eacute; uma propriedade importante da complexidade de Kolmogorov.</p>&#10;<a id="Seq%C3%BC%C3%AAncias_Aleat%C3%B3rias_de_Martin&shy;L%C3%B6f" name="Seq%C3%BC%C3%AAncias_Aleat%C3%B3rias_de_Martin&shy;L%C3%B6f"></a><h2> Seq&uuml;&ecirc;ncias Aleat&oacute;rias de Martin&shy;L&ouml;f </h2>&#10;<p>Uma resposta consistente ao problema de definir o conceito de seq&uuml;&ecirc;ncia aleat&oacute;ria foi dada por <a href="http://en.wikipedia.org/wiki/Per_Martin&shy;L%C3%B6f" id="w">Per Martin&shy;L&ouml;f</a>. A defini&ccedil;&atilde;o de Martin&shy;L&ouml;f baseia&shy;se em <a href="http://en.wikipedia.org/wiki/Teoria_da_medida" id="w">teoria da medida</a>, e na exist&ecirc;ncia de um conjunto efetivo nulo maximal, no conjunto das strings bin&aacute;rias. Isto significa a exist&ecirc;ncia de um conjunto nulo (com baixa probabilidade de ocorr&ecirc;ncia) que cont&eacute;m todos os conjuntos nulos cujas strings podem ser algoritmicamente geradas. Esta defini&ccedil;&atilde;o equivale a afirmar a exist&ecirc;ncia de um teste de aleatoriedade universal. Interessante observar que a defini&ccedil;&atilde;o de Martin&shy;L&ouml;f &eacute; equivalente &agrave; defini&ccedil;&atilde;o de seq&uuml;&ecirc;ncia aleat&oacute;ria dada pela complexidade de Kolmogorov (via incompressividade das strings).</p>&#10;<a id="Aplica%C3%A7%C3%B5es" name="Aplica%C3%A7%C3%B5es"></a><h2> Aplica&ccedil;&otilde;es </h2>&#10;<p>Solomonoff prop&ocirc;s o uso da regra de Bayes para obter previs&atilde;o indutiva, ou seja, para prever a seq&uuml;&ecirc;ncia de uma string bin&aacute;ria. Para isto ele usou como probabilidade pr&eacute;via a probabilidade universal, que pode ser definida como <span class="math">2^{&shy;K(x)}</span>, porque ela domina toda probabilidade pr&eacute;via semi&shy;comput&aacute;vel conceb&iacute;vel. Isto constitui&shy;se no n&uacute;cleo dos m&eacute;todos de intelig&ecirc;ncia artificial MDL (minimum description length) e MML (minimum message length).</p>&#10;<p>A complexidade <span class="math">K(x|y)</span> induz um conceito de dist&acirc;ncia (ou similaridade), chamada dist&acirc;ncia de informa&ccedil;&atilde;o, que &eacute; uma medida a priori e absoluta sobre o conjunto das strings bin&aacute;rias. Esta dist&acirc;ncia pode ser aplicada em diversos e diferentes contextos de forma muito similar a outras medidas de dist&acirc;ncia definidas na matem&aacute;tica. O interessante &eacute; que podemos aproximar esta medida usando um programa compressor de dados e efetuar medi&ccedil;&otilde;es emp&iacute;ricas. Destacam&shy;se como aplica&ccedil;&otilde;es desta dist&acirc;ncia o reconhecimento de genoma, a classifica&ccedil;&atilde;o autom&aacute;tica de m&uacute;sica, e o estabelecimento de uma hierarquia de linguagens humanas.</p>&#10;<p>Chaitin construiu um paradoxo envolvendo o tamanho dos programas que constitui&shy;se em uma prova alternativa ao que ficou conhecido como prova de <a href="http://en.wikipedia.org/wiki/Kurt_G%C3%B6del" id="w">G&ouml;del</a> (ou <a href="http://en.wikipedia.org/wiki/Teorema_da_incompletude_de_G%C3%B6del" id="w">Teorema da incompletude de G&ouml;del</a>). Chaitin baseou&shy;se no paradoxo de Berry que sup&otilde;e considerar o menor n&uacute;mero inteiro positivo que n&atilde;o pode ser definido por uma frase em portugu&ecirc;s com menos que 1.000.000.000 de caracteres. No entanto, a pr&oacute;pria defini&ccedil;&atilde;o do problema define o n&uacute;mero e tem menos de 1.000.000.000 de caracteres, o que &eacute; uma contradi&ccedil;&atilde;o. Isto resulta que strings n&atilde;o podem ser produzidas por programas que tenham menos complexidade que a pr&oacute;pria string, sendo isto um limite dos sistemas formais.</p>&#10;<p>Uma outra interessante aplica&ccedil;&atilde;o de complexidade de Kolmogorov &eacute; o n&uacute;mero m&aacute;gico e m&iacute;stico <span class="math">\Omega</span>, proposto por Chaitin, definido como: <span class="math">\Omega=\sum_p 2^{&shy;|p|} .</span> Nesta f&oacute;rmula, <span class="math">p</span> representa um programa que p&aacute;ra (finaliza) e <span class="math">|p|</span> &eacute; o tamanho deste programa. &Eacute; interessante observar que <span class="math">0\leq \Omega\leq 1</span>, representando <span class="math">\Omega</span> a probabilidade de um programa qualquer parar (finalizar). <span class="math">\Omega</span> &eacute; um n&uacute;mero real aleat&oacute;rio (cujos d&iacute;gitos formam uma seq&uuml;&ecirc;ncia aleat&oacute;ria) e n&atilde;o comput&aacute;vel (ou seja, n&atilde;o pode ser computado por um programa na m&aacute;quina de Turing). Al&eacute;m disto, <span class="math">\Omega</span> cont&eacute;m em si, da forma mais compacta poss&iacute;vel, todas as verdades matem&aacute;ticas poss&iacute;veis de serem expressas.</p>&#10;<p>Diversas outras aplica&ccedil;&otilde;es da teoria foram desenvolvidas desde ent&atilde;o: <a href="http://en.wikipedia.org/wiki/Intelig%C3%AAncia_artificial" id="w">intelig&ecirc;ncia artificial</a>, <a href="http://en.wikipedia.org/wiki/Linguagens_formais" id="w">linguagens formais</a>, <a href="http://en.wikipedia.org/wiki/Complexidade_computacional" id="w">complexidade computacional</a>, <a href="http://en.wikipedia.org/wiki/Teoria_dos_grafos" id="w">teoria dos grafos</a>, <a href="http://en.wikipedia.org/wiki/Biotecnologia" id="w">biotecnologia</a>, etc.</p>&#10;<a id="%7B%7BVeja_tamb%C3%A9m%7D%7D" name="%7B%7BVeja_tamb%C3%A9m%7D%7D"></a><h2>   </h2>&#10;&#10;<ul>&#10;<li><a href="http://en.wikipedia.org/wiki/Andrei_Nikolaevich_Kolmogorov" id="w">Andrei Nikolaevich Kolmogorov</a></li>&#10;<li><a href="http://en.wikipedia.org/wiki/Teoria_da_informa%C3%A7%C3%A3o" id="w">Teoria da informa&ccedil;&atilde;o</a></li>&#10;<li><a href="http://en.wikipedia.org/wiki/Teoria_das_probabilidades" id="w">Teoria das probabilidades</a></li>&#10;<li><a href="http://en.wikipedia.org/wiki/Probabilidade" id="w">Probabilidade</a></li>&#10;<li><a href="http://en.wikipedia.org/wiki/Aleatoriedade" id="w">Aleatoriedade</a></li>&#10;<li><a href="http://en.wikipedia.org/wiki/Teoria_da_computa%C3%A7%C3%A3o" id="w">Teoria da computa&ccedil;&atilde;o</a></li>&#10;<li><a href="http://en.wikipedia.org/wiki/M%C3%A1quina_de_Turing" id="w">M&aacute;quina de Turing</a></li></ul>&#10;<a id="%7B%7BLinks_externos%7D%7D" name="%7B%7BLinks_externos%7D%7D"></a><h2>   </h2>&#10;&#10;<ul>&#10;<li><a class="externallink" href="http://www.hutter1.de/kolmo.htm" rel="nofollow" title="http://www.hutter1.de/kolmo.htm">Kolmogorov Complexity Resources</a></li>&#10;<li><a class="externallink" href="http://www.umcs.maine.edu/~chaitin/" rel="nofollow" title="http://www.umcs.maine.edu/~chaitin/">Gregory Chaitin home page</a></li>&#10;<li><a class="externallink" href="http://homepages.cwi.nl/~paulv/index.html" rel="nofollow" title="http://homepages.cwi.nl/~paulv/index.html">Paul Vit&aacute;nyi home page</a></li>&#10;<li><a class="externallink" href="http://world.std.com/~rjs/ray.html" rel="nofollow" title="http://world.std.com/~rjs/ray.html">Ray Solomonoff home page</a></li>&#10;<li><a class="externallink" href="http://www&shy;groups.dcs.st&shy;and.ac.uk/~history/Mathematicians/Kolmogorov.html" rel="nofollow" title="http://www&shy;groups.dcs.st&shy;and.ac.uk/~history/Mathematicians/Kolmogorov.html">Kolmogorov Biography</a></li></ul>&#10;<p><a href="http://en.wikipedia.org/wiki/Categoria:Teoria_do_caos" id="w">Categoria:Teoria do caos</a></p></body></html>